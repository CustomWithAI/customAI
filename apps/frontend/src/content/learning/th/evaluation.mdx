---
title: ตัวชี้วัดการประเมินโมเดล
description: ทำความเข้าใจ Accuracy, Precision, Recall, F1 Score และ Confusion Matrix สำหรับการจำแนกประเภทใน ML และ DL
date: 2025-05-02
author: Documentation Team
tags: [AI, model]
---

# การวัดผลโมเดลใน Machine Learning และ Deep Learning

การวัดผลโมเดลเป็นขั้นตอนที่สำคัญในการประเมินประสิทธิภาพและความแม่นยำของโมเดลที่พัฒนา เพื่อให้แน่ใจว่าโมเดลสามารถทำงานได้ตามที่คาดหวังเมื่อเผชิญกับข้อมูลใหม่

## ทำไมต้องวัดผลโมเดล?

การประเมินโมเดลช่วยให้เราทราบถึงความสามารถในการทำนายหรือจำแนกข้อมูล รวมถึงการระบุข้อบกพร่องหรือจุดที่ควรปรับปรุง เพื่อให้โมเดลสามารถใช้งานได้อย่างมีประสิทธิภาพ

## การจำแนกประเภท

### Accuracy: สัดส่วนของการทำนายที่ถูกต้องต่อการทำนายทั้งหมด
Accuracy เป็นตัวชี้วัดสำคัญในการประเมินประสิทธิภาพของโมเดลการเรียนรู้ของเครื่อง โดยเฉพาะในงานจำแนกประเภท มันแสดงถึงสัดส่วนของการทำนายที่ถูกต้องเมื่อเทียบกับจำนวนการทำนายทั้งหมด

**สูตรการคำนวณ:**
Accuracy = การทำนายที่ถูกต้อง / การทำนายทั้งหมด

**ตัวอย่าง:**
หากโมเดลทำนายทั้งหมด 100 ครั้ง และการทำนายที่ถูกต้องมี 90 ครั้ง ค่า Accuracy จะเป็น:
Accuracy = 90 / 100 = 0.9 หรือ 90%

**การตีความค่า Accuracy:**
- **ค่า Accuracy สูง:** บ่งชี้ว่าโมเดลมีความสามารถในการทำนายได้ถูกต้องในสัดส่วนที่สูง ซึ่งเป็นสิ่งที่พึงประสงค์
- **ค่า Accuracy ต่ำ:** บ่งชี้ว่าโมเดลมีการทำนายที่ผิดพลาดมาก ซึ่งอาจต้องปรับปรุง

**ข้อควรระวัง:**
ในกรณีที่มีข้อมูลที่ไม่สมดุลระหว่างคลาส ค่า Accuracy อาจไม่สะท้อนถึงประสิทธิภาพที่แท้จริงของโมเดลได้

### Precision: สัดส่วนของการทำนายที่ถูกต้องที่เป็นบวกต่อการทำนายที่เป็นบวกทั้งหมด
Precision เป็นตัวชี้วัดที่ใช้ในการประเมินประสิทธิภาพของโมเดล โดยเฉพาะในงานจำแนกประเภท ซึ่งแสดงถึงความสามารถในการทำนายผลบวกที่ถูกต้องเมื่อเทียบกับผลทำนายทั้งหมดที่เป็นบวก

**สูตรการคำนวณ:**
Precision = True Positive / (True Positive + False Positive)

**ตัวอย่าง:**
สมมติว่าโมเดลทำนายอีเมล 100 ฉบับเป็นสแปม และในนั้นมี 80 ฉบับที่เป็นสแปมจริง (True Positive) และ 20 ฉบับที่ไม่ใช่สแปมแต่ทำนายผิดว่าเป็นสแปม (False Positive) ค่า Precision จะเป็น:
Precision = 80 / (80 + 20) = 0.8 หรือ 80%

**การตีความค่า Precision:**
- **ค่า Precision สูง:** บ่งชี้ว่าเมื่อโมเดลทำนายว่าเป็นผลบวก (เช่น สแปม) มักจะถูกต้อง
- **ค่า Precision ต่ำ:** บ่งชี้ว่าโมเดลทำนายผิดพลาดบ่อยครั้ง

### Recall: สัดส่วนของการทำนายที่เป็นบวกที่ถูกต้องต่อกรณีที่เป็นบวกทั้งหมด
Recall เป็นตัวชี้วัดที่ใช้ในการประเมินความสามารถของโมเดลในการระบุกรณีบวกทั้งหมด

**สูตรการคำนวณ:**
Recall = True Positive / (True Positive + False Negative)

**ตัวอย่าง:**
สมมติว่าในกลุ่มอีเมลสแปมมีทั้งหมด 100 ฉบับ และโมเดลสามารถทำนายได้ถูกต้อง 80 ฉบับ (True Positive) และพลาดไป 20 ฉบับ (False Negative) ค่า Recall จะเป็น:
Recall = 80 / (80 + 20) = 0.8 หรือ 80%

**การตีความค่า Recall:**
- **ค่า Recall สูง:** บ่งชี้ว่าโมเดลสามารถระบุกรณีบวกได้เกือบทั้งหมด
- **ค่า Recall ต่ำ:** บ่งชี้ว่าโมเดลพลาดกรณีบวกหลายกรณี

### F1 Score: ค่าเฉลี่ยเชิงฮาร์มอนิกของ Precision และ Recall
F1 Score เป็นตัวชี้วัดที่รวม Precision และ Recall เข้าด้วยกัน ซึ่งใช้ประเมินโมเดลในกรณีที่มีการแลกเปลี่ยนกันระหว่าง Precision และ Recall

**สูตรการคำนวณ:**
F1 Score = 2 * (Precision * Recall) / (Precision + Recall)

**ตัวอย่าง:**
หาก Precision = 0.75 และ Recall = 0.60 ค่า F1 Score จะเป็น:
F1 Score = 2 * (0.75 * 0.60) / (0.75 + 0.60) = 0.67 หรือ 67%

### Confusion Matrix: ตารางแสดงจำนวนการทำนายที่ถูกต้องและผิดพลาดสำหรับแต่ละคลาส
Confusion Matrix เป็นเครื่องมือที่ใช้ในการประเมินโมเดลการจำแนกประเภท โดยเปรียบเทียบการทำนายคลาสกับคลาสจริงเพื่อแสดงจำนวนการทำนายที่ถูกต้องและผิดพลาด

**โครงสร้างของ Confusion Matrix:**
สำหรับการจำแนกประเภทแบบทวิภาค (binary classification) ประกอบด้วย:

- **True Positive (TP):** การทำนายที่ถูกต้องในคลาสบวก
- **False Positive (FP):** การทำนายผิดในคลาสบวก
- **False Negative (FN):** การทำนายผิดในคลาสลบ
- **True Negative (TN):** การทำนายที่ถูกต้องในคลาสลบ

### การตรวจจับวัตถุ

#### Mean Average Precision (mAP): ค่าเฉลี่ยของ Average Precision (AP) สำหรับแต่ละคลาส
Mean Average Precision (mAP) เป็นตัวชี้วัดที่ใช้ในการประเมินประสิทธิภาพของโมเดลในการตรวจจับวัตถุ ซึ่งคำนวณจากค่า Average Precision (AP) ของแต่ละคลาส โดยที่ AP คือพื้นที่ใต้กราฟ Precision-Recall

